FIXES:

ONLY DROP NA for columns that im using - ie: dont drop rows lacking P+245d for no reason.

monte-carlo conditionals after initial +/- binary ~10-30d classifier?
-update daily w/ new OHLCV movements? minimize p(loss) and E[loss]


5 — Decision & trading rule ideas
Screening rule: select trades with P(event in 15d) > expected value thresh.
Dynamic exit: for a screened trade, use the conditional time-to-peak distribution to adaptively set a stop / take-profit: e.g., hold until predicted median time-to-peak, or until realized return > dynamic threshold.
back: plot distribution of trade returns (make sure im not getting carried/dunked by outliers)

7 — Suggested experiment roadmap (in order)
Implement binary classifier for any positive in X=15 days using LightGBM
Evaluate classifier (AUC, Precision@k). If useful, compute expected value using a simple magnitude regressor (mean mag per bin).
Build second-stage regression: on rows with y=1, predict time_to_peak or magnitude_at_peak. Use AFT or RSF for time or LightGBM for magnitude.
Combine: for each candidate compute E(return by t) = P(event by t) * E[magnitude at event | predict t] and simulate simple strategy.
Upgrade: discrete-time hazard model or RSF if you need more accurate timing. Try DeepHit if you have plenty of data and engineering resources.


F — Two-stage approach (recommended pragmatic path)
Stage 1: Binary classifier or hazard model predicting P(event within X) or hazard curve.
Stage 2: For rows predicted likely-to-event, fit a conditional model that predicts either time-to-event (AFT / RSF on the subset) or magnitude at event (regression).
Pros: modular, interpretable, flexible; lets you control which subset you commit capital to.
Cons: needs careful cross-validation to avoid information leakage across stages.

G — Multi-task / multi-horizon (single model predicting multiple horizons)
Train model to predict ret_p_p1, ret_p_p2, ..., ret_p_pX simultaneously (multioutput GBM or multihead NN).
Pros: shared representation can improve sample efficiency; you get an expected path of returns.
Cons: more complex; needs careful loss weighting.

Add- dif model idea - predict time til profitability? like survival curve/ratio/etc model? worth a try instead of fixed predictions. need to figure out how to assess model strength and reliability tho/variance.

bruh def double check the nearest/date algo. make conservative. 

maybe employ a LLM agent to do the missing ticker searches?
ie: for each that yf doesn't find:
	-search 'investing.com full_corp_name + TICKER stock price history'
	-pick the first/most likely investing.com link on google. copy that url. if no links are remotely likely, throw some sort of error so i go check manually
or try to dload by getting the actual base js api that ivcom draws from, and hitting that with the requests module from python

Add filter before model gen:
4) remove all that dont exist (ie: STNL was delisted in 2019 but had a weird insider blip in 2022 trade, 2023 filing - ez drop)

- what duration to hold?
	-figure out combo of:
		 likelihood of going positive 
		 estimated positive return magnitude
to really remove risk of downside:
	-1 run a logistic classifier/regression on pos vs neg returns, optimize
	-2 AND run a linreg/multilinreg on those to est how big pos/neg returns
	-then pick only those who POS(1) and STRONG POS(2)
(check but lowkey thats what i think i currently have)
later: dynamic meta-model that updates hold duration? see if that actually improves accuracy lol against base case