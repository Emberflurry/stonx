{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db870df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\John DeForest\\AppData\\Local\\Temp\\ipykernel_87968\\1005754476.py:56: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[c] = pd.to_datetime(df[c], errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\John DeForest\\AppData\\Local\\Temp\\ipykernel_87968\\1005754476.py:56: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[c] = pd.to_datetime(df[c], errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\John DeForest\\AppData\\Local\\Temp\\ipykernel_87968\\1005754476.py:56: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[c] = pd.to_datetime(df[c], errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\John DeForest\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['sig_c_1' 'sig_gk_1' 'sig_gkc_1']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\John DeForest\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['sig_c_1' 'sig_gk_1' 'sig_gkc_1']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\John DeForest\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['sig_c_1' 'sig_gk_1' 'sig_gkc_1']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 227\u001b[39m\n\u001b[32m    222\u001b[39m X_train_imp = imputer.fit_transform(X_train)\n\u001b[32m    223\u001b[39m gbr = GradientBoostingRegressor(\n\u001b[32m    224\u001b[39m     random_state=\u001b[32m42\u001b[39m, n_estimators=\u001b[32m500\u001b[39m, learning_rate=\u001b[32m0.05\u001b[39m,\n\u001b[32m    225\u001b[39m     max_depth=\u001b[32m3\u001b[39m, subsample=\u001b[32m0.8\u001b[39m\n\u001b[32m    226\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m \u001b[43mgbr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_imp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m X_k = panel_bi.loc[panel_bi[\u001b[33m\"\u001b[39m\u001b[33mt\u001b[39m\u001b[33m\"\u001b[39m] == k, numeric_features]\n\u001b[32m    230\u001b[39m X_k_imp = imputer.transform(X_k)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\ensemble\\_gb.py:787\u001b[39m, in \u001b[36mBaseGradientBoosting.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, monitor)\u001b[39m\n\u001b[32m    784\u001b[39m     \u001b[38;5;28mself\u001b[39m._resize_state()\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m n_stages = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_stages != \u001b[38;5;28mself\u001b[39m.estimators_.shape[\u001b[32m0\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\ensemble\\_gb.py:883\u001b[39m, in \u001b[36mBaseGradientBoosting._fit_stages\u001b[39m\u001b[34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[39m\n\u001b[32m    876\u001b[39m         initial_loss = factor * \u001b[38;5;28mself\u001b[39m._loss(\n\u001b[32m    877\u001b[39m             y_true=y_oob_masked,\n\u001b[32m    878\u001b[39m             raw_prediction=raw_predictions[~sample_mask],\n\u001b[32m    879\u001b[39m             sample_weight=sample_weight_oob_masked,\n\u001b[32m    880\u001b[39m         )\n\u001b[32m    882\u001b[39m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m883\u001b[39m raw_predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\ensemble\\_gb.py:489\u001b[39m, in \u001b[36mBaseGradientBoosting._fit_stage\u001b[39m\u001b[34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[39m\n\u001b[32m    486\u001b[39m     sample_weight = sample_weight * sample_mask.astype(np.float64)\n\u001b[32m    488\u001b[39m X = X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_g_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[32m    494\u001b[39m X_for_tree_update = X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\tree\\_classes.py:1404\u001b[39m, in \u001b[36mDecisionTreeRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m   1374\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, check_input=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1376\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[32m   1377\u001b[39m \n\u001b[32m   1378\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1401\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1402\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1404\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#DO NOT RUN LOCALLY - @65min STILL RUNNING\n",
    "# ================================================================\n",
    "# Optimal-Stopping MDP Panel Builder + Backward Induction Targets\n",
    "# ================================================================\n",
    "# - Expands each trade into daily decision states t=1..H (default H=10)\n",
    "# - Guarantees NO LEAKAGE: only uses info available up to time t\n",
    "# - Computes SELL (immediate exercise) payoff at each t\n",
    "# - Runs backward induction to produce V_target (no risk adjustments)\n",
    "# - Saves:\n",
    "#     1) mdp_trajectory_panel.csv  (full panel w/ identifiers)\n",
    "#     2) mdp_features_for_value_model.csv (numeric features for model)\n",
    "# ================================================================\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# -------------------\n",
    "# Config\n",
    "# -------------------\n",
    "# >>> EDIT THIS <<<\n",
    "INPUT_PATH = \"oip_mega_boost1.csv\"   # your big CSV\n",
    "# or, if you're testing the provided example Excel:\n",
    "# INPUT_PATH = r\"/mnt/data/oip_mega_boost1_example_r30859.xlsx\"\n",
    "\n",
    "# Output locations\n",
    "OUTPUT_PANEL_CSV    = \"mdp_trajectory_panel.csv\"\n",
    "OUTPUT_FEATURES_CSV = \"mdp_features_for_value_model.csv\"\n",
    "\n",
    "# Horizon (max days after entry to consider)\n",
    "H_DEFAULT = 10\n",
    "\n",
    "# Train/Test splits by entry date (mebuydate)\n",
    "TRAIN_END  = pd.Timestamp(\"2024-02-21\")\n",
    "TEST_START = pd.Timestamp(\"2024-02-21\")\n",
    "TEST_END   = pd.Timestamp(\"2025-07-22\")\n",
    "\n",
    "# -------------------\n",
    "# Load\n",
    "# -------------------\n",
    "def load_any(INPUT_PATH: str) -> pd.DataFrame:\n",
    "    if INPUT_PATH.lower().endswith(\".xlsx\"):\n",
    "        df0 = pd.read_excel(INPUT_PATH)\n",
    "    else:\n",
    "        # Fast CSV read; tweak dtype/options as needed for your file\n",
    "        df0 = pd.read_csv(INPUT_PATH)\n",
    "    return df0\n",
    "\n",
    "df = load_any(INPUT_PATH).copy()\n",
    "\n",
    "# Parse dates (mm/dd/yyyy or m/d/yyyy tolerated)\n",
    "for c in [\"filing_date\", \"trade_date\", \"mebuydate\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_datetime(df[c], errors=\"coerce\", infer_datetime_format=True)\n",
    "\n",
    "# -------------------\n",
    "# Identify forward-looking columns (must NOT be used for features at time t)\n",
    "# -------------------\n",
    "re_price_fwd = re.compile(r\"^p_p(\\d+)_td$\")        # e.g., p_p1_td (price at +1 TD)\n",
    "re_vol_fwd   = re.compile(r\"^v_p(\\d+)_td$\")        # e.g., v_p3_td (volume at +3 TD)\n",
    "re_ret_fwd   = re.compile(r\"^ret_p_p(\\d+)_td$\")    # e.g., ret_p_p5_td (entry->+5 TD return)\n",
    "\n",
    "fwd_price_cols = sorted([(int(re_price_fwd.match(c).group(1)), c) for c in df.columns if re_price_fwd.match(c)])\n",
    "fwd_vol_cols   = sorted([(int(re_vol_fwd.match(c).group(1)), c)   for c in df.columns if re_vol_fwd.match(c)])\n",
    "fwd_ret_cols   = sorted([(int(re_ret_fwd.match(c).group(1)), c)   for c in df.columns if re_ret_fwd.match(c)])\n",
    "\n",
    "max_fwd = 0\n",
    "if fwd_price_cols: max_fwd = max(max_fwd, max(k for k,_ in fwd_price_cols))\n",
    "if fwd_vol_cols:   max_fwd = max(max_fwd,   max(k for k,_ in fwd_vol_cols))\n",
    "if fwd_ret_cols:   max_fwd = max(max_fwd,   max(k for k,_ in fwd_ret_cols))\n",
    "H = min(H_DEFAULT, max_fwd) if max_fwd > 0 else H_DEFAULT\n",
    "\n",
    "def is_forward_col(c: str) -> bool:\n",
    "    return bool(re_price_fwd.match(c) or re_vol_fwd.match(c) or re_ret_fwd.match(c))\n",
    "\n",
    "# -------------------\n",
    "# Base features (safe at entry): exclude forward-look columns\n",
    "# We'll keep id-like columns separate (not in the model feature set)\n",
    "# -------------------\n",
    "id_like = {\n",
    "    \"ticker\",\"company_name\",\"insider_name\",\"title\",\"trade_type\",\"year\",\n",
    "    \"filing_date\",\"trade_date\",\"mebuydate\",\"mebuy_price\",\"filing_price\"\n",
    "}\n",
    "\n",
    "base_feature_cols = []\n",
    "for c in df.columns:\n",
    "    if is_forward_col(c):\n",
    "        continue\n",
    "    if c in id_like:\n",
    "        continue\n",
    "    base_feature_cols.append(c)\n",
    "\n",
    "# -------------------\n",
    "# Time-varying features up to (and including) day t\n",
    "# Uses only info allowed as-of t\n",
    "# -------------------\n",
    "def build_time_features(row: pd.Series, t: int) -> dict:\n",
    "    feats = {}\n",
    "    # cumulative return to day t, if present (ret_p_p{t}_td)\n",
    "    ret_col = f\"ret_p_p{t}_td\"\n",
    "    price_col_t   = f\"p_p{t}_td\"\n",
    "    price_col_tm1 = f\"p_p{t-1}_td\" if t-1 >= 1 else None\n",
    "\n",
    "    ret_t = row.get(ret_col, np.nan)\n",
    "    px_t  = row.get(price_col_t, np.nan)\n",
    "    px_tm1 = row.get(price_col_tm1, np.nan) if price_col_tm1 else np.nan\n",
    "    entry_px = row.get(\"mebuy_price\", np.nan)\n",
    "\n",
    "    # Cum ret to day t\n",
    "    feats[\"cum_ret_t\"] = ret_t\n",
    "\n",
    "    # One-day return (t vs t-1), or (t vs entry) for t=1\n",
    "    if t == 1:\n",
    "        if pd.notna(px_t) and pd.notna(entry_px) and entry_px != 0:\n",
    "            feats[\"ret_1d\"] = px_t / entry_px - 1.0\n",
    "        else:\n",
    "            feats[\"ret_1d\"] = ret_t if pd.notna(ret_t) else np.nan\n",
    "    else:\n",
    "        if pd.notna(px_t) and pd.notna(px_tm1) and px_tm1 != 0:\n",
    "            feats[\"ret_1d\"] = px_t / px_tm1 - 1.0\n",
    "        else:\n",
    "            feats[\"ret_1d\"] = np.nan\n",
    "\n",
    "    # (Optional) You can extend here with additional *as-of-t* features,\n",
    "    # e.g., path-dependent stats built only from info up to t.\n",
    "    return feats\n",
    "\n",
    "# -------------------\n",
    "# Build the trajectory panel\n",
    "# -------------------\n",
    "panel_rows = []\n",
    "df_reset = df.reset_index(drop=True)\n",
    "\n",
    "for idx, row in df_reset.iterrows():\n",
    "    trade_id = idx\n",
    "    entry_dt = row.get(\"mebuydate\", pd.NaT)\n",
    "    entry_px = row.get(\"mebuy_price\", np.nan)\n",
    "    ticker   = row.get(\"ticker\", None)\n",
    "\n",
    "    for t in range(1, H+1):\n",
    "        price_t = row.get(f\"p_p{t}_td\", np.nan)\n",
    "        ret_t   = row.get(f\"ret_p_p{t}_td\", np.nan)\n",
    "\n",
    "        # Skip if we don't have *either* the price or the return for day t\n",
    "        if pd.isna(price_t) and pd.isna(ret_t):\n",
    "            continue\n",
    "\n",
    "        # Assemble base & time-varying features\n",
    "        base_feats = row[base_feature_cols].to_dict()\n",
    "        tv_feats   = build_time_features(row, t)\n",
    "        days_left  = H - t\n",
    "\n",
    "        # Immediate SELL payoff at time t (relative to entry)\n",
    "        payoff_t = ret_t\n",
    "        if pd.isna(payoff_t) and pd.notna(price_t) and pd.notna(entry_px) and entry_px != 0:\n",
    "            payoff_t = price_t / entry_px - 1.0\n",
    "\n",
    "        panel_rows.append({\n",
    "            \"trade_id\": trade_id,\n",
    "            \"ticker\": ticker,\n",
    "            \"entry_date\": entry_dt,\n",
    "            \"t\": t,\n",
    "            \"days_left\": days_left,\n",
    "            \"payoff_t\": payoff_t,\n",
    "            \"SELL_value\": payoff_t,\n",
    "            **base_feats,\n",
    "            **tv_feats\n",
    "        })\n",
    "\n",
    "panel = pd.DataFrame(panel_rows)\n",
    "\n",
    "# -------------------\n",
    "# Train/Test split by entry_date\n",
    "# -------------------\n",
    "def label_set(d: pd.Timestamp) -> str:\n",
    "    if pd.isna(d):\n",
    "        return \"drop\"\n",
    "    if d <= TRAIN_END:\n",
    "        return \"train\"\n",
    "    if (d >= TEST_START) and (d <= TEST_END):\n",
    "        return \"test\"\n",
    "    return \"drop\"\n",
    "\n",
    "panel[\"set\"] = panel[\"entry_date\"].apply(label_set)\n",
    "panel = panel[panel[\"set\"] != \"drop\"].copy()\n",
    "\n",
    "# -------------------\n",
    "# Backward induction to derive V_target (no risk adjustments)\n",
    "# - numeric-only features + median imputation for the learner\n",
    "# -------------------\n",
    "panel_bi = panel.copy()\n",
    "panel_bi[\"V_target\"] = np.nan\n",
    "\n",
    "# Numeric-only feature matrix (exclude identifiers / target-like fields)\n",
    "exclude_feats = {\"trade_id\",\"ticker\",\"entry_date\",\"set\",\"V_target\",\"payoff_t\",\"SELL_value\"}\n",
    "numeric_cols = panel_bi.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_features = [c for c in numeric_cols if c not in exclude_feats]\n",
    "\n",
    "# Ensure time coordinates included\n",
    "if \"t\" not in numeric_features:\n",
    "    numeric_features = [\"t\"] + numeric_features\n",
    "if \"days_left\" not in numeric_features:\n",
    "    numeric_features = [\"days_left\"] + [c for c in numeric_features if c != \"days_left\"]\n",
    "\n",
    "# Initialize terminal step: must sell at t=H\n",
    "panel_bi.loc[panel_bi[\"t\"] == H, \"V_target\"] = panel_bi.loc[panel_bi[\"t\"] == H, \"payoff_t\"]\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Work backwards t = H-1 ... 1\n",
    "for k in range(H-1, 0, -1):\n",
    "    train_mask = panel_bi[\"t\"] >= (k+1)\n",
    "    X_train = panel_bi.loc[train_mask, numeric_features]\n",
    "    y_train = panel_bi.loc[train_mask, \"V_target\"]\n",
    "\n",
    "    # If insufficient data or missing targets (e.g., very sparse tails), fallback to SELL_value\n",
    "    if (train_mask.sum() < 50) or y_train.isna().any():\n",
    "        hold_pred = panel_bi.loc[panel_bi[\"t\"] == k, \"payoff_t\"].values\n",
    "    else:\n",
    "        X_train_imp = imputer.fit_transform(X_train)\n",
    "        gbr = GradientBoostingRegressor(\n",
    "            random_state=42, n_estimators=500, learning_rate=0.05,\n",
    "            max_depth=3, subsample=0.8\n",
    "        )\n",
    "        gbr.fit(X_train_imp, y_train.values)\n",
    "\n",
    "        X_k = panel_bi.loc[panel_bi[\"t\"] == k, numeric_features]\n",
    "        X_k_imp = imputer.transform(X_k)\n",
    "        hold_pred = gbr.predict(X_k_imp)\n",
    "\n",
    "    sell_val = panel_bi.loc[panel_bi[\"t\"] == k, \"SELL_value\"].values\n",
    "    v_k = np.maximum(sell_val, hold_pred)\n",
    "    panel_bi.loc[panel_bi[\"t\"] == k, \"V_target\"] = v_k\n",
    "\n",
    "# -------------------\n",
    "# Save outputs\n",
    "# -------------------\n",
    "# 1) Full panel with identifiers and V_target (good for auditing & backtesting)\n",
    "panel_bi.to_csv(OUTPUT_PANEL_CSV, index=False)\n",
    "\n",
    "# 2) Trimmed numeric feature table for modeling\n",
    "#    Keep identifiers + set + SELL_value + V_target + numeric_features\n",
    "meta_cols = [\"trade_id\",\"ticker\",\"entry_date\",\"t\",\"days_left\",\"set\",\"SELL_value\",\"V_target\"]\n",
    "keep_cols = meta_cols + [c for c in numeric_features if c not in {\"t\",\"days_left\"}]  # t/days_left already in meta\n",
    "# Ensure uniqueness and preserve order\n",
    "seen = set()\n",
    "ordered_keep = []\n",
    "for c in keep_cols:\n",
    "    if c not in seen and c in panel_bi.columns:\n",
    "        ordered_keep.append(c)\n",
    "        seen.add(c)\n",
    "\n",
    "feat_df = panel_bi[ordered_keep].copy()\n",
    "feat_df.to_csv(OUTPUT_FEATURES_CSV, index=False)\n",
    "\n",
    "print(f\"Done. Horizon H={H}\")\n",
    "print(f\"Panel rows: {len(panel_bi):,}\")\n",
    "print(f\"Train rows: {int((panel_bi['set']=='train').sum()):,}\")\n",
    "print(f\"Test rows : {int((panel_bi['set']=='test').sum()):,}\")\n",
    "print(f\"Wrote: {OUTPUT_PANEL_CSV}\")\n",
    "print(f\"Wrote: {OUTPUT_FEATURES_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
