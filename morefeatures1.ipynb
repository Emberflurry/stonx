{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1be06e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowKeyError",
     "evalue": "No type extension with name arrow.py_extension_type found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowKeyError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 334\u001b[39m\n\u001b[32m    330\u001b[39m     \u001b[38;5;28mprint\u001b[39m(df[[\u001b[33m\"\u001b[39m\u001b[33mticker\u001b[39m\u001b[33m\"\u001b[39m, DATE_COL_TRADE, DATE_COL_FILE, \u001b[33m\"\u001b[39m\u001b[33mtrade2file_td\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    331\u001b[39m               \u001b[33m\"\u001b[39m\u001b[33mret_trade_to_mebuy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpurchase_pct_mcap\u001b[39m\u001b[33m\"\u001b[39m]].head(\u001b[32m10\u001b[39m))\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 266\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(input_csv, output_csv)\u001b[39m\n\u001b[32m    264\u001b[39m max_d = end_dates.max()\n\u001b[32m    265\u001b[39m tickers = \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(df[\u001b[33m\"\u001b[39m\u001b[33mticker\u001b[39m\u001b[33m\"\u001b[39m].dropna().astype(\u001b[38;5;28mstr\u001b[39m).str.upper()))\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m closes = \u001b[43mbatch_yf_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtickers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_d\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTimedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdays\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m80\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpause\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m closes.columns = [c.upper() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m closes.columns]\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# --- Feature 2: ret_trade_to_mebuy\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[38;5;66;03m# lookup trade close & mebuy close (fallback mebuy->filing_date when NaT)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 206\u001b[39m, in \u001b[36mbatch_yf_download\u001b[39m\u001b[34m(tickers, start, end, batch_size, pause)\u001b[39m\n\u001b[32m    204\u001b[39m     out = closes \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m out.empty \u001b[38;5;28;01melse\u001b[39;00m out.join(closes, how=\u001b[33m\"\u001b[39m\u001b[33mouter\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    205\u001b[39m     out = out.sort_index()\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[43mappend_prices_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcloses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m     time.sleep(pause)  \u001b[38;5;66;03m# be nice to Yahoo\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 156\u001b[39m, in \u001b[36mappend_prices_cache\u001b[39m\u001b[34m(new_df)\u001b[39m\n\u001b[32m    154\u001b[39m old = load_prices_cache()\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m old \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[43msave_prices_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m new_df\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# union columns & index\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 148\u001b[39m, in \u001b[36msave_prices_cache\u001b[39m\u001b[34m(df_prices)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_prices_cache\u001b[39m(df_prices):\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[43mdf_prices\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPRICES_CACHE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\frame.py:3118\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3037\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3038\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3039\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3114\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3115\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3116\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3118\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3119\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parquet.py:478\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    477\u001b[39m     partition_cols = [partition_cols]\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m    482\u001b[39m impl.write(\n\u001b[32m    483\u001b[39m     df,\n\u001b[32m    484\u001b[39m     path_or_buf,\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m     **kwargs,\n\u001b[32m    491\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parquet.py:64\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m engine_class \u001b[38;5;129;01min\u001b[39;00m engine_classes:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     66\u001b[39m         error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parquet.py:170\u001b[39m, in \u001b[36mPyArrowImpl.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;28mself\u001b[39m.api = pyarrow\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\arrays\\arrow\\extension_types.py:174\u001b[39m\n\u001b[32m    167\u001b[39m     pyarrow.register_extension_type(\n\u001b[32m    168\u001b[39m         ForbiddenExtensionType(pyarrow.null(), \u001b[33m\"\u001b[39m\u001b[33marrow.py_extension_type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m     )\n\u001b[32m    171\u001b[39m     pyarrow._hotfix_installed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[43mpatch_pyarrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\arrays\\arrow\\extension_types.py:166\u001b[39m, in \u001b[36mpatch_pyarrow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    157\u001b[39m         pickletools.dis(serialized, out)\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    159\u001b[39m             _ERROR_MSG.format(\n\u001b[32m    160\u001b[39m                 storage_type=storage_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m    163\u001b[39m             )\n\u001b[32m    164\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[43mpyarrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43munregister_extension_type\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marrow.py_extension_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m pyarrow.register_extension_type(\n\u001b[32m    168\u001b[39m     ForbiddenExtensionType(pyarrow.null(), \u001b[33m\"\u001b[39m\u001b[33marrow.py_extension_type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m )\n\u001b[32m    171\u001b[39m pyarrow._hotfix_installed = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyarrow\\types.pxi:2280\u001b[39m, in \u001b[36mpyarrow.lib.unregister_extension_type\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowKeyError\u001b[39m: No type extension with name arrow.py_extension_type found"
     ]
    }
   ],
   "source": [
    "# pip install pandas numpy yfinance pandas_market_calendars pyarrow\n",
    "import os, math, json, time, warnings\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# --------- CONFIG ---------\n",
    "INPUT_CSV   = \"oip_mega_wretvol.csv\"\n",
    "OUTPUT_CSV  = \"oip_mega_boost1.csv\"\n",
    "DATE_COL_TRADE  = \"trade_date\"\n",
    "DATE_COL_FILE   = \"filing_date\"\n",
    "DATE_COL_MEBUY  = \"mebuydate\"       # optional; falls back to filing_date if missing\n",
    "TICKER_COL      = \"ticker\"\n",
    "\n",
    "# Optional: drop rows with excessive delay (set to None to keep all)\n",
    "MAX_TD_DELAY = None  # e.g., 10\n",
    "\n",
    "# Value/price columns (first available is used)\n",
    "VALUE_COL_CANDIDATES = [\"value\", \"transaction_value\", \"amount_usd\", \"amount\"]\n",
    "PRICE_COL_CANDIDATES = [\"price\", \"avg_price\", \"price_per_share\", \"transaction_price\", \"px\"]\n",
    "\n",
    "# --------- CACHE ---------\n",
    "CACHE_DIR     = \"cache\"\n",
    "PRICES_CACHE  = os.path.join(CACHE_DIR, \"prices.parquet\")\n",
    "SHARES_CACHE  = os.path.join(CACHE_DIR, \"shares.json\")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# --------- UTILS ---------\n",
    "def to_dt(s):\n",
    "    if pd.isna(s):\n",
    "        return pd.NaT\n",
    "    return pd.to_datetime(s, errors=\"coerce\").normalize()\n",
    "\n",
    "def first_existing_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def ensure_cols(df, needed):\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "def normalize_to_yahoo_symbol(x: str) -> str:\n",
    "    # Yahoo uses '-' instead of '.' for share classes (e.g., BRK.B -> BRK-B)\n",
    "    return str(x).strip().upper().replace(\".\", \"-\")\n",
    "\n",
    "def get_trading_days_delay(trade_dates, file_dates):\n",
    "    try:\n",
    "        import pandas_market_calendars as pmc\n",
    "        nyse = pmc.get_calendar(\"XNYS\")\n",
    "        start = min(trade_dates.min(), file_dates.min())\n",
    "        end   = max(trade_dates.max(), file_dates.max())\n",
    "        sched = nyse.schedule(start_date=start - pd.Timedelta(days=10),\n",
    "                              end_date=end + pd.Timedelta(days=10))\n",
    "        trading_days = nyse.valid_days(schedule=sched).tz_localize(None).normalize()\n",
    "        trading_days = pd.Index(trading_days)\n",
    "        pos = pd.Series(np.arange(len(trading_days), dtype=\"int64\"), index=trading_days)\n",
    "        trade_pos = trade_dates.map(lambda d: pos.get(d, np.nan))\n",
    "        file_pos  = file_dates.map(lambda d: pos.get(d, np.nan))\n",
    "        td = (file_pos - trade_pos).astype(\"Float64\")\n",
    "        return td.round().astype(\"Int64\")\n",
    "    except Exception:\n",
    "        # Weekday-only fallback\n",
    "        t = trade_dates.values.astype(\"datetime64[D]\")\n",
    "        f = file_dates.values.astype(\"datetime64[D]\")\n",
    "        return pd.Series(np.busday_count(t, f), index=trade_dates.index, dtype=\"int64\")\n",
    "\n",
    "# ---- replace parquet cache with pickle cache ----\n",
    "PRICES_CACHE = os.path.join(CACHE_DIR, \"prices.pkl\")  # change extension\n",
    "\n",
    "def load_prices_cache():\n",
    "    try:\n",
    "        return pd.read_pickle(PRICES_CACHE) if os.path.exists(PRICES_CACHE) else None\n",
    "    except Exception:\n",
    "        return None  # corrupt/old cache -> ignore\n",
    "\n",
    "def save_prices_cache(df_prices):\n",
    "    df_prices.to_pickle(PRICES_CACHE)\n",
    "\n",
    "def append_prices_cache(new_df):\n",
    "    old = load_prices_cache()\n",
    "    if old is None or old.empty:\n",
    "        save_prices_cache(new_df)\n",
    "        return new_df\n",
    "    # union columns & index, prefer new non-nulls\n",
    "    df = old.combine_first(new_df).join(new_df, how=\"outer\", rsuffix=\"_new\")\n",
    "    for c in new_df.columns:\n",
    "        cn = c + \"_new\"\n",
    "        if cn in df.columns:\n",
    "            df[c] = df[c].fillna(df[cn])\n",
    "            df.drop(columns=[cn], inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    save_prices_cache(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_shares_cache():\n",
    "    return json.load(open(SHARES_CACHE)) if os.path.exists(SHARES_CACHE) else {}\n",
    "\n",
    "def save_shares_cache(d):\n",
    "    with open(SHARES_CACHE, \"w\") as f:\n",
    "        json.dump(d, f)\n",
    "\n",
    "def batch_yf_download(tickers, start, end, batch_size=80, pause=1.5):\n",
    "    want = sorted(set(tickers))\n",
    "    cached = load_prices_cache()\n",
    "    need = want if cached is None else [t for t in want if t not in cached.columns]\n",
    "    out = cached if cached is not None else pd.DataFrame()\n",
    "\n",
    "    for i in range(0, len(need), batch_size):\n",
    "        chunk = need[i:i+batch_size]\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                data = yf.download(\n",
    "                    tickers=chunk, start=start, end=end + pd.Timedelta(days=2),\n",
    "                    auto_adjust=True, progress=False, group_by=\"ticker\", threads=True\n",
    "                )\n",
    "                break\n",
    "            except Exception:\n",
    "                time.sleep(2**attempt)\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            closes = data.xs(\"Close\", axis=1, level=1)\n",
    "        else:\n",
    "            closes = pd.DataFrame({chunk[0]: data.get(\"Close\")})\n",
    "        closes.index = pd.to_datetime(closes.index).normalize()\n",
    "        closes = closes.dropna(how=\"all\").sort_index()\n",
    "\n",
    "        out = closes if out is None or out.empty else out.join(closes, how=\"outer\")\n",
    "        out = out.sort_index()\n",
    "        append_prices_cache(closes)\n",
    "        print(f\"Downloaded {i+len(chunk)}/{len(need)} tickers…\")\n",
    "        time.sleep(pause)\n",
    "    return out\n",
    "\n",
    "def fetch_one_shares(t):\n",
    "    so = np.nan\n",
    "    try:\n",
    "        tk = yf.Ticker(t)\n",
    "        try:\n",
    "            so = getattr(tk.fast_info, \"shares_outstanding\", None)\n",
    "        except Exception:\n",
    "            so = None\n",
    "        if not so:\n",
    "            info = tk.info or {}\n",
    "            so = info.get(\"sharesOutstanding\") or info.get(\"impliedSharesOutstanding\")\n",
    "        return t, float(so) if so else np.nan\n",
    "    except Exception:\n",
    "        return t, np.nan\n",
    "\n",
    "def get_shares_outstanding_cached(tickers, max_workers=8):\n",
    "    cache = load_shares_cache()\n",
    "    todo = [t for t in tickers if t not in cache or cache[t] in (None, 0, \"nan\")]\n",
    "    if not todo:\n",
    "        return cache\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs = [ex.submit(fetch_one_shares, t) for t in todo]\n",
    "        for n, f in enumerate(as_completed(futs), 1):\n",
    "            t, so = f.result()\n",
    "            cache[t] = so\n",
    "            if n % 100 == 0:\n",
    "                print(f\"Shares fetched: {n}/{len(todo)}\")\n",
    "    save_shares_cache(cache)\n",
    "    return cache\n",
    "\n",
    "def nearest_previous_close(closes_df, date, ticker):\n",
    "    if closes_df is None or closes_df.empty or pd.isna(date) or ticker not in closes_df.columns:\n",
    "        return np.nan\n",
    "    # exact\n",
    "    try:\n",
    "        if date in closes_df.index:\n",
    "            v = closes_df.at[date, ticker]\n",
    "            if pd.notna(v):\n",
    "                return float(v)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # previous available\n",
    "    loc = closes_df.index.searchsorted(date, side=\"right\") - 1\n",
    "    while loc >= 0:\n",
    "        v = closes_df.iloc[loc][ticker]\n",
    "        if pd.notna(v):\n",
    "            return float(v)\n",
    "        loc -= 1\n",
    "    return np.nan\n",
    "\n",
    "# --------- MAIN PIPELINE ---------\n",
    "def main(input_csv=INPUT_CSV, output_csv=OUTPUT_CSV):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    ensure_cols(df, [TICKER_COL, DATE_COL_TRADE, DATE_COL_FILE])\n",
    "\n",
    "    # Dates\n",
    "    df[DATE_COL_TRADE] = pd.to_datetime(df[DATE_COL_TRADE], errors=\"coerce\").dt.normalize()\n",
    "    df[DATE_COL_FILE]  = pd.to_datetime(df[DATE_COL_FILE],  errors=\"coerce\").dt.normalize()\n",
    "    if DATE_COL_MEBUY in df.columns:\n",
    "        df[DATE_COL_MEBUY] = pd.to_datetime(df[DATE_COL_MEBUY], errors=\"coerce\").dt.normalize()\n",
    "    else:\n",
    "        df[DATE_COL_MEBUY] = pd.NaT\n",
    "\n",
    "    # Tickers → Yahoo format\n",
    "    df[TICKER_COL] = df[TICKER_COL].astype(str).map(normalize_to_yahoo_symbol)\n",
    "    tickers = sorted(set(df[TICKER_COL].dropna()))\n",
    "\n",
    "    # Feature 1: trading‑day delay\n",
    "    df[\"trade2file_td\"] = get_trading_days_delay(df[DATE_COL_TRADE], df[DATE_COL_FILE])\n",
    "    # Optional: clip absurd negatives (bad data) and warn\n",
    "    bad = df[\"trade2file_td\"] < 0\n",
    "    if bad.any():\n",
    "        warnings.warn(f\"{bad.sum()} rows have filing before trade (negative delay). Clipping to 0.\")\n",
    "        df.loc[bad, \"trade2file_td\"] = 0\n",
    "\n",
    "    # Optional filter by max delay\n",
    "    if isinstance(MAX_TD_DELAY, (int, float)) and MAX_TD_DELAY >= 0:\n",
    "        before = len(df)\n",
    "        df = df[df[\"trade2file_td\"].astype(\"float64\") <= MAX_TD_DELAY].copy()\n",
    "        print(f\"Filtered by MAX_TD_DELAY={MAX_TD_DELAY}: {before} -> {len(df)} rows\")\n",
    "\n",
    "    # Price window\n",
    "    end_dates = df[DATE_COL_MEBUY].fillna(df[DATE_COL_FILE])\n",
    "    min_d = df[DATE_COL_TRADE].min()\n",
    "    max_d = end_dates.max()\n",
    "\n",
    "    # Batch download closes (resumable cache)\n",
    "    closes = batch_yf_download(tickers, min_d - pd.Timedelta(days=5), max_d, batch_size=80, pause=1.2)\n",
    "    if closes is None or closes.empty:\n",
    "        warnings.warn(\"No prices downloaded; ret_trade_to_mebuy will be NaN.\")\n",
    "\n",
    "    # Column names normalized\n",
    "    if closes is not None and not closes.empty:\n",
    "        closes.columns = [normalize_to_yahoo_symbol(c) for c in closes.columns]\n",
    "\n",
    "    # Feature 2: ret_trade_to_mebuy\n",
    "    trade_px = np.empty(len(df)); trade_px[:] = np.nan\n",
    "    mebuy_px = np.empty(len(df)); mebuy_px[:] = np.nan\n",
    "\n",
    "    for i, (tkr, d_trade, d_mebuy, d_file) in enumerate(zip(\n",
    "        df[TICKER_COL].values,\n",
    "        df[DATE_COL_TRADE].values,\n",
    "        df[DATE_COL_MEBUY].values,\n",
    "        df[DATE_COL_FILE].values\n",
    "    )):\n",
    "        d_target = d_mebuy if pd.notna(d_mebuy) else d_file\n",
    "        trade_px[i] = nearest_previous_close(closes, d_trade, tkr)\n",
    "        mebuy_px[i] = nearest_previous_close(closes, d_target, tkr)\n",
    "        if (i+1) % 5000 == 0:\n",
    "            print(f\"Prices looked up for {i+1} rows…\")\n",
    "\n",
    "    df[\"px_trade_close\"] = trade_px\n",
    "    df[\"px_mebuy_close\"] = mebuy_px\n",
    "    df[\"ret_trade_to_mebuy\"] = np.where(\n",
    "        (df[\"px_trade_close\"] > 0) & (df[\"px_mebuy_close\"] > 0),\n",
    "        df[\"px_mebuy_close\"] / df[\"px_trade_close\"] - 1.0,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Feature 3: purchase_pct_mcap\n",
    "    value_col = first_existing_col(df, VALUE_COL_CANDIDATES)\n",
    "    price_col = first_existing_col(df, PRICE_COL_CANDIDATES)\n",
    "\n",
    "    # Coerce to numeric safely\n",
    "    def num(s): return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "    if value_col is not None:\n",
    "        purchase_value = num(df[value_col])\n",
    "    else:\n",
    "        qty_col = \"qty\" if \"qty\" in df.columns else None\n",
    "        if qty_col and price_col:\n",
    "            purchase_value = num(df[qty_col]) * num(df[price_col])\n",
    "        else:\n",
    "            purchase_value = pd.Series(np.nan, index=df.index, dtype=\"float64\")\n",
    "\n",
    "    # Shares outstanding (cached, non‑historical approximation)\n",
    "    shares_map = get_shares_outstanding_cached(tickers, max_workers=8)\n",
    "    df[\"shares_outstanding_curr\"] = df[TICKER_COL].map(shares_map)\n",
    "    df[\"mcap_trade_approx\"] = df[\"px_trade_close\"] * df[\"shares_outstanding_curr\"]\n",
    "\n",
    "    df[\"purchase_pct_mcap\"] = np.where(\n",
    "        (df[\"mcap_trade_approx\"] > 0) & (purchase_value > 0),\n",
    "        100.0 * purchase_value / df[\"mcap_trade_approx\"],\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    nan_rate = df[\"purchase_pct_mcap\"].isna().mean()\n",
    "    if nan_rate > 0.25:\n",
    "        warnings.warn(f\"'purchase_pct_mcap' NaN ratio = {nan_rate:.0%}. \"\n",
    "                      f\"Likely missing prices or shares for some tickers.\")\n",
    "\n",
    "    # Save\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Enriched {len(df):,} rows -> {output_csv}\")\n",
    "    print(df[[TICKER_COL, DATE_COL_TRADE, DATE_COL_FILE, \"trade2file_td\",\n",
    "              \"ret_trade_to_mebuy\", \"purchase_pct_mcap\"]].head(8))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
