{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c1be06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\John DeForest\\AppData\\Local\\Temp\\ipykernel_45420\\2788973031.py:211: UserWarning: 7 rows have filing before trade (negative delay). Clipping to 0.\n",
      "  warnings.warn(f\"{bad.sum()} rows have filing before trade (negative delay). Clipping to 0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 80/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['AKTS']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 160/4183 tickers…\n",
      "Downloaded 240/4183 tickers…\n",
      "Downloaded 320/4183 tickers…\n",
      "Downloaded 400/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['BFI']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 480/4183 tickers…\n",
      "Downloaded 560/4183 tickers…\n",
      "Downloaded 640/4183 tickers…\n",
      "Downloaded 720/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['CCTS']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 800/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['CMAX']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 880/4183 tickers…\n",
      "Downloaded 960/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2 Failed downloads:\n",
      "['CSSE', 'CSWI']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1040/4183 tickers…\n",
      "Downloaded 1120/4183 tickers…\n",
      "Downloaded 1200/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['EIGR']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1280/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2 Failed downloads:\n",
      "['EVA', 'EVGRU']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1360/4183 tickers…\n",
      "Downloaded 1440/4183 tickers…\n",
      "Downloaded 1520/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['FUV']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1600/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['GMDA']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1680/4183 tickers…\n",
      "Downloaded 1760/4183 tickers…\n",
      "Downloaded 1840/4183 tickers…\n",
      "Downloaded 1920/4183 tickers…\n",
      "Downloaded 2000/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['KACLU']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2080/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['LBAI']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2160/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['LIAN']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2240/4183 tickers…\n",
      "Downloaded 2320/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['ME']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2400/4183 tickers…\n",
      "Downloaded 2480/4183 tickers…\n",
      "Downloaded 2560/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['NOVA']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2640/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2 Failed downloads:\n",
      "['NSTG', 'OB']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2720/4183 tickers…\n",
      "Downloaded 2800/4183 tickers…\n",
      "Downloaded 2880/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['PMD']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2960/4183 tickers…\n",
      "Downloaded 3040/4183 tickers…\n",
      "Downloaded 3120/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['RGF']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 3200/4183 tickers…\n",
      "Downloaded 3280/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['SHPW']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 3360/4183 tickers…\n",
      "Downloaded 3440/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['SRCL']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 3520/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['STER']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 3600/4183 tickers…\n",
      "Downloaded 3680/4183 tickers…\n",
      "Downloaded 3760/4183 tickers…\n",
      "Downloaded 3840/4183 tickers…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['VIEW']: YFPricesMissingError('possibly delisted; no price data found  (1d 2002-04-27 00:00:00 -> 2025-08-08 00:00:00) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 3920/4183 tickers…\n",
      "Downloaded 4000/4183 tickers…\n",
      "Downloaded 4080/4183 tickers…\n",
      "Downloaded 4160/4183 tickers…\n",
      "Downloaded 4183/4183 tickers…\n",
      "Prices looked up for 5000 rows…\n",
      "Prices looked up for 10000 rows…\n",
      "Prices looked up for 15000 rows…\n",
      "Prices looked up for 20000 rows…\n",
      "Prices looked up for 25000 rows…\n",
      "Prices looked up for 30000 rows…\n",
      "Prices looked up for 35000 rows…\n",
      "Prices looked up for 40000 rows…\n",
      "Prices looked up for 45000 rows…\n",
      "Prices looked up for 50000 rows…\n",
      "Prices looked up for 55000 rows…\n",
      "Prices looked up for 60000 rows…\n",
      "Prices looked up for 65000 rows…\n",
      "Prices looked up for 70000 rows…\n",
      "Shares fetched: 100/4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares fetched: 200/4183\n",
      "Shares fetched: 300/4183\n",
      "Shares fetched: 400/4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares fetched: 500/4183\n",
      "Shares fetched: 600/4183\n",
      "Shares fetched: 700/4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares fetched: 800/4183\n",
      "Shares fetched: 900/4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: \n",
      "HTTP Error 404: \n",
      "HTTP Error 401: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares fetched: 1000/4183\n",
      "Shares fetched: 1100/4183\n",
      "Shares fetched: 1200/4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares fetched: 1300/4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares fetched: 1400/4183\n",
      "Shares fetched: 1500/4183\n",
      "Shares fetched: 1600/4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares fetched: 1700/4183\n",
      "Shares fetched: 1800/4183\n",
      "Shares fetched: 1900/4183\n",
      "Shares fetched: 2000/4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares fetched: 2100/4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 404: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares fetched: 2200/4183\n",
      "Shares fetched: 2300/4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares fetched: 2400/4183\n",
      "Shares fetched: 2500/4183\n",
      "Shares fetched: 2600/4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares fetched: 2700/4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares fetched: 2800/4183\n",
      "Shares fetched: 2900/4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares fetched: 3000/4183\n",
      "Shares fetched: 3100/4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares fetched: 3200/4183\n",
      "Shares fetched: 3300/4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares fetched: 3400/4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares fetched: 3500/4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n",
      "HTTP Error 401: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares fetched: 3600/4183\n",
      "Shares fetched: 3700/4183\n",
      "Shares fetched: 3800/4183\n",
      "Shares fetched: 3900/4183\n",
      "Shares fetched: 4000/4183\n",
      "Shares fetched: 4100/4183\n",
      "Enriched 73,494 rows -> oip_mega_boost1.csv\n",
      "  ticker trade_date filing_date  trade2file_td  ret_trade_to_mebuy  \\\n",
      "0   PINE 2025-08-01  2025-08-05              2            0.004954   \n",
      "1    FIG 2025-08-01  2025-08-05              2           -0.259672   \n",
      "2    FIG 2025-08-01  2025-08-05              2           -0.259672   \n",
      "3    FIG 2025-08-01  2025-08-05              2           -0.259672   \n",
      "4    FIG 2025-08-01  2025-08-05              2           -0.259672   \n",
      "5   AMKR 2025-08-01  2025-08-05              2            0.021149   \n",
      "6   AMKR 2025-08-01  2025-08-05              2            0.021149   \n",
      "7   AMKR 2025-08-01  2025-08-05              2            0.021149   \n",
      "\n",
      "   purchase_pct_mcap  \n",
      "0           0.281666  \n",
      "1           0.004147  \n",
      "2           0.010616  \n",
      "3           0.001990  \n",
      "4           0.003981  \n",
      "5           0.179499  \n",
      "6           0.179499  \n",
      "7           0.179499  \n"
     ]
    }
   ],
   "source": [
    "# pip install pandas numpy yfinance pandas_market_calendars pyarrow\n",
    "import os, math, json, time, warnings\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# --------- CONFIG ---------\n",
    "INPUT_CSV   = \"oip_mega_wretvol.csv\"\n",
    "OUTPUT_CSV  = \"oip_mega_boost1.csv\"\n",
    "DATE_COL_TRADE  = \"trade_date\"\n",
    "DATE_COL_FILE   = \"filing_date\"\n",
    "DATE_COL_MEBUY  = \"mebuydate\"       # optional; falls back to filing_date if missing\n",
    "TICKER_COL      = \"ticker\"\n",
    "\n",
    "# Optional: drop rows with excessive delay (set to None to keep all)\n",
    "MAX_TD_DELAY = None  # e.g., 10\n",
    "\n",
    "# Value/price columns (first available is used)\n",
    "VALUE_COL_CANDIDATES = [\"value\", \"transaction_value\", \"amount_usd\", \"amount\"]\n",
    "PRICE_COL_CANDIDATES = [\"price\", \"avg_price\", \"price_per_share\", \"transaction_price\", \"px\"]\n",
    "\n",
    "# --------- CACHE ---------\n",
    "CACHE_DIR     = \"cache\"\n",
    "PRICES_CACHE  = os.path.join(CACHE_DIR, \"prices.parquet\")\n",
    "SHARES_CACHE  = os.path.join(CACHE_DIR, \"shares.json\")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# --------- UTILS ---------\n",
    "def to_dt(s):\n",
    "    if pd.isna(s):\n",
    "        return pd.NaT\n",
    "    return pd.to_datetime(s, errors=\"coerce\").normalize()\n",
    "\n",
    "def first_existing_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def ensure_cols(df, needed):\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "def normalize_to_yahoo_symbol(x: str) -> str:\n",
    "    # Yahoo uses '-' instead of '.' for share classes (e.g., BRK.B -> BRK-B)\n",
    "    return str(x).strip().upper().replace(\".\", \"-\")\n",
    "\n",
    "def get_trading_days_delay(trade_dates, file_dates):\n",
    "    try:\n",
    "        import pandas_market_calendars as pmc\n",
    "        nyse = pmc.get_calendar(\"XNYS\")\n",
    "        start = min(trade_dates.min(), file_dates.min())\n",
    "        end   = max(trade_dates.max(), file_dates.max())\n",
    "        sched = nyse.schedule(start_date=start - pd.Timedelta(days=10),\n",
    "                              end_date=end + pd.Timedelta(days=10))\n",
    "        trading_days = nyse.valid_days(schedule=sched).tz_localize(None).normalize()\n",
    "        trading_days = pd.Index(trading_days)\n",
    "        pos = pd.Series(np.arange(len(trading_days), dtype=\"int64\"), index=trading_days)\n",
    "        trade_pos = trade_dates.map(lambda d: pos.get(d, np.nan))\n",
    "        file_pos  = file_dates.map(lambda d: pos.get(d, np.nan))\n",
    "        td = (file_pos - trade_pos).astype(\"Float64\")\n",
    "        return td.round().astype(\"Int64\")\n",
    "    except Exception:\n",
    "        # Weekday-only fallback\n",
    "        t = trade_dates.values.astype(\"datetime64[D]\")\n",
    "        f = file_dates.values.astype(\"datetime64[D]\")\n",
    "        return pd.Series(np.busday_count(t, f), index=trade_dates.index, dtype=\"int64\")\n",
    "\n",
    "# ---- replace parquet cache with pickle cache ----\n",
    "PRICES_CACHE = os.path.join(CACHE_DIR, \"prices.pkl\")  # change extension\n",
    "\n",
    "def load_prices_cache():\n",
    "    try:\n",
    "        return pd.read_pickle(PRICES_CACHE) if os.path.exists(PRICES_CACHE) else None\n",
    "    except Exception:\n",
    "        return None  # corrupt/old cache -> ignore\n",
    "\n",
    "def save_prices_cache(df_prices):\n",
    "    df_prices.to_pickle(PRICES_CACHE)\n",
    "\n",
    "def append_prices_cache(new_df):\n",
    "    old = load_prices_cache()\n",
    "    if old is None or old.empty:\n",
    "        save_prices_cache(new_df)\n",
    "        return new_df\n",
    "    # union columns & index, prefer new non-nulls\n",
    "    df = old.combine_first(new_df).join(new_df, how=\"outer\", rsuffix=\"_new\")\n",
    "    for c in new_df.columns:\n",
    "        cn = c + \"_new\"\n",
    "        if cn in df.columns:\n",
    "            df[c] = df[c].fillna(df[cn])\n",
    "            df.drop(columns=[cn], inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    save_prices_cache(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_shares_cache():\n",
    "    return json.load(open(SHARES_CACHE)) if os.path.exists(SHARES_CACHE) else {}\n",
    "\n",
    "def save_shares_cache(d):\n",
    "    with open(SHARES_CACHE, \"w\") as f:\n",
    "        json.dump(d, f)\n",
    "\n",
    "def batch_yf_download(tickers, start, end, batch_size=80, pause=1.5):\n",
    "    want = sorted(set(tickers))\n",
    "    cached = load_prices_cache()\n",
    "    need = want if cached is None else [t for t in want if t not in cached.columns]\n",
    "    out = cached if cached is not None else pd.DataFrame()\n",
    "\n",
    "    for i in range(0, len(need), batch_size):\n",
    "        chunk = need[i:i+batch_size]\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                data = yf.download(\n",
    "                    tickers=chunk, start=start, end=end + pd.Timedelta(days=2),\n",
    "                    auto_adjust=True, progress=False, group_by=\"ticker\", threads=True\n",
    "                )\n",
    "                break\n",
    "            except Exception:\n",
    "                time.sleep(2**attempt)\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            closes = data.xs(\"Close\", axis=1, level=1)\n",
    "        else:\n",
    "            closes = pd.DataFrame({chunk[0]: data.get(\"Close\")})\n",
    "        closes.index = pd.to_datetime(closes.index).normalize()\n",
    "        closes = closes.dropna(how=\"all\").sort_index()\n",
    "\n",
    "        out = closes if out is None or out.empty else out.join(closes, how=\"outer\")\n",
    "        out = out.sort_index()\n",
    "        append_prices_cache(closes)\n",
    "        print(f\"Downloaded {i+len(chunk)}/{len(need)} tickers…\")\n",
    "        time.sleep(pause)\n",
    "    return out\n",
    "\n",
    "def fetch_one_shares(t):\n",
    "    so = np.nan\n",
    "    try:\n",
    "        tk = yf.Ticker(t)\n",
    "        try:\n",
    "            so = getattr(tk.fast_info, \"shares_outstanding\", None)\n",
    "        except Exception:\n",
    "            so = None\n",
    "        if not so:\n",
    "            info = tk.info or {}\n",
    "            so = info.get(\"sharesOutstanding\") or info.get(\"impliedSharesOutstanding\")\n",
    "        return t, float(so) if so else np.nan\n",
    "    except Exception:\n",
    "        return t, np.nan\n",
    "\n",
    "def get_shares_outstanding_cached(tickers, max_workers=8):\n",
    "    cache = load_shares_cache()\n",
    "    todo = [t for t in tickers if t not in cache or cache[t] in (None, 0, \"nan\")]\n",
    "    if not todo:\n",
    "        return cache\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs = [ex.submit(fetch_one_shares, t) for t in todo]\n",
    "        for n, f in enumerate(as_completed(futs), 1):\n",
    "            t, so = f.result()\n",
    "            cache[t] = so\n",
    "            if n % 100 == 0:\n",
    "                print(f\"Shares fetched: {n}/{len(todo)}\")\n",
    "    save_shares_cache(cache)\n",
    "    return cache\n",
    "\n",
    "def nearest_previous_close(closes_df, date, ticker):\n",
    "    if closes_df is None or closes_df.empty or pd.isna(date) or ticker not in closes_df.columns:\n",
    "        return np.nan\n",
    "    # exact\n",
    "    try:\n",
    "        if date in closes_df.index:\n",
    "            v = closes_df.at[date, ticker]\n",
    "            if pd.notna(v):\n",
    "                return float(v)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # previous available\n",
    "    loc = closes_df.index.searchsorted(date, side=\"right\") - 1\n",
    "    while loc >= 0:\n",
    "        v = closes_df.iloc[loc][ticker]\n",
    "        if pd.notna(v):\n",
    "            return float(v)\n",
    "        loc -= 1\n",
    "    return np.nan\n",
    "\n",
    "# --------- MAIN PIPELINE ---------\n",
    "def main(input_csv=INPUT_CSV, output_csv=OUTPUT_CSV):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    ensure_cols(df, [TICKER_COL, DATE_COL_TRADE, DATE_COL_FILE])\n",
    "\n",
    "    # Dates\n",
    "    df[DATE_COL_TRADE] = pd.to_datetime(df[DATE_COL_TRADE], errors=\"coerce\").dt.normalize()\n",
    "    df[DATE_COL_FILE]  = pd.to_datetime(df[DATE_COL_FILE],  errors=\"coerce\").dt.normalize()\n",
    "    if DATE_COL_MEBUY in df.columns:\n",
    "        df[DATE_COL_MEBUY] = pd.to_datetime(df[DATE_COL_MEBUY], errors=\"coerce\").dt.normalize()\n",
    "    else:\n",
    "        df[DATE_COL_MEBUY] = pd.NaT\n",
    "\n",
    "    # Tickers → Yahoo format\n",
    "    df[TICKER_COL] = df[TICKER_COL].astype(str).map(normalize_to_yahoo_symbol)\n",
    "    tickers = sorted(set(df[TICKER_COL].dropna()))\n",
    "\n",
    "    # Feature 1: trading‑day delay\n",
    "    df[\"trade2file_td\"] = get_trading_days_delay(df[DATE_COL_TRADE], df[DATE_COL_FILE])\n",
    "    # Optional: clip absurd negatives (bad data) and warn\n",
    "    bad = df[\"trade2file_td\"] < 0\n",
    "    if bad.any():\n",
    "        warnings.warn(f\"{bad.sum()} rows have filing before trade (negative delay). Clipping to 0.\")\n",
    "        df.loc[bad, \"trade2file_td\"] = 0\n",
    "\n",
    "    # Optional filter by max delay\n",
    "    if isinstance(MAX_TD_DELAY, (int, float)) and MAX_TD_DELAY >= 0:\n",
    "        before = len(df)\n",
    "        df = df[df[\"trade2file_td\"].astype(\"float64\") <= MAX_TD_DELAY].copy()\n",
    "        print(f\"Filtered by MAX_TD_DELAY={MAX_TD_DELAY}: {before} -> {len(df)} rows\")\n",
    "\n",
    "    # Price window\n",
    "    end_dates = df[DATE_COL_MEBUY].fillna(df[DATE_COL_FILE])\n",
    "    min_d = df[DATE_COL_TRADE].min()\n",
    "    max_d = end_dates.max()\n",
    "\n",
    "    # Batch download closes (resumable cache)\n",
    "    closes = batch_yf_download(tickers, min_d - pd.Timedelta(days=5), max_d, batch_size=80, pause=1.2)\n",
    "    if closes is None or closes.empty:\n",
    "        warnings.warn(\"No prices downloaded; ret_trade_to_mebuy will be NaN.\")\n",
    "\n",
    "    # Column names normalized\n",
    "    if closes is not None and not closes.empty:\n",
    "        closes.columns = [normalize_to_yahoo_symbol(c) for c in closes.columns]\n",
    "\n",
    "    # Feature 2: ret_trade_to_mebuy\n",
    "    trade_px = np.empty(len(df)); trade_px[:] = np.nan\n",
    "    mebuy_px = np.empty(len(df)); mebuy_px[:] = np.nan\n",
    "\n",
    "    for i, (tkr, d_trade, d_mebuy, d_file) in enumerate(zip(\n",
    "        df[TICKER_COL].values,\n",
    "        df[DATE_COL_TRADE].values,\n",
    "        df[DATE_COL_MEBUY].values,\n",
    "        df[DATE_COL_FILE].values\n",
    "    )):\n",
    "        d_target = d_mebuy if pd.notna(d_mebuy) else d_file\n",
    "        trade_px[i] = nearest_previous_close(closes, d_trade, tkr)\n",
    "        mebuy_px[i] = nearest_previous_close(closes, d_target, tkr)\n",
    "        if (i+1) % 5000 == 0:\n",
    "            print(f\"Prices looked up for {i+1} rows…\")\n",
    "\n",
    "    df[\"px_trade_close\"] = trade_px\n",
    "    df[\"px_mebuy_close\"] = mebuy_px\n",
    "    df[\"ret_trade_to_mebuy\"] = np.where(\n",
    "        (df[\"px_trade_close\"] > 0) & (df[\"px_mebuy_close\"] > 0),\n",
    "        df[\"px_mebuy_close\"] / df[\"px_trade_close\"] - 1.0,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Feature 3: purchase_pct_mcap\n",
    "    value_col = first_existing_col(df, VALUE_COL_CANDIDATES)\n",
    "    price_col = first_existing_col(df, PRICE_COL_CANDIDATES)\n",
    "\n",
    "    # Coerce to numeric safely\n",
    "    def num(s): return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "    if value_col is not None:\n",
    "        purchase_value = num(df[value_col])\n",
    "    else:\n",
    "        qty_col = \"qty\" if \"qty\" in df.columns else None\n",
    "        if qty_col and price_col:\n",
    "            purchase_value = num(df[qty_col]) * num(df[price_col])\n",
    "        else:\n",
    "            purchase_value = pd.Series(np.nan, index=df.index, dtype=\"float64\")\n",
    "\n",
    "    # Shares outstanding (cached, non‑historical approximation)\n",
    "    shares_map = get_shares_outstanding_cached(tickers, max_workers=8)\n",
    "    df[\"shares_outstanding_curr\"] = df[TICKER_COL].map(shares_map)\n",
    "    df[\"mcap_trade_approx\"] = df[\"px_trade_close\"] * df[\"shares_outstanding_curr\"]\n",
    "\n",
    "    df[\"purchase_pct_mcap\"] = np.where(\n",
    "        (df[\"mcap_trade_approx\"] > 0) & (purchase_value > 0),\n",
    "        100.0 * purchase_value / df[\"mcap_trade_approx\"],\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    nan_rate = df[\"purchase_pct_mcap\"].isna().mean()\n",
    "    if nan_rate > 0.25:\n",
    "        warnings.warn(f\"'purchase_pct_mcap' NaN ratio = {nan_rate:.0%}. \"\n",
    "                      f\"Likely missing prices or shares for some tickers.\")\n",
    "\n",
    "    # Save\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Enriched {len(df):,} rows -> {output_csv}\")\n",
    "    print(df[[TICKER_COL, DATE_COL_TRADE, DATE_COL_FILE, \"trade2file_td\",\n",
    "              \"ret_trade_to_mebuy\", \"purchase_pct_mcap\"]].head(8))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
